{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images /= 255\n",
    "test_images /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample(num):\n",
    "    #Printing the one-hot array of this sample's label \n",
    "    print(train_labels[num])  \n",
    "    #Printing the label converted back to a number\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    #Reshape the 768 values to a 28x28 image\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', input_shape = input_shape))\n",
    "\n",
    "# 64 3x3 kernels\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu',))\n",
    "\n",
    "#reducing by taking the maximum of each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size =(2, 2)))\n",
    "\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# a hidden layer to learn with\n",
    "model.add(Dense(128, activation ='relu'))\n",
    "\n",
    "# Dropout (preserving from overfitting)\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#final categorization from 0-9 with Softmax\n",
    "model.add(Dense(10,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 66s - loss: 0.1811 - acc: 0.9451 - val_loss: 0.0406 - val_acc: 0.9859\n",
      "Epoch 2/10\n",
      " - 52s - loss: 0.0747 - acc: 0.9783 - val_loss: 0.0350 - val_acc: 0.9883\n",
      "Epoch 3/10\n",
      " - 52s - loss: 0.0562 - acc: 0.9832 - val_loss: 0.0308 - val_acc: 0.9901\n",
      "Epoch 4/10\n",
      " - 52s - loss: 0.0408 - acc: 0.9879 - val_loss: 0.0319 - val_acc: 0.9898\n",
      "Epoch 5/10\n",
      " - 52s - loss: 0.0359 - acc: 0.9887 - val_loss: 0.0348 - val_acc: 0.9899\n",
      "Epoch 6/10\n",
      " - 52s - loss: 0.0291 - acc: 0.9906 - val_loss: 0.0274 - val_acc: 0.9916\n",
      "Epoch 7/10\n",
      " - 52s - loss: 0.0231 - acc: 0.9925 - val_loss: 0.0304 - val_acc: 0.9917\n",
      "Epoch 8/10\n",
      " - 53s - loss: 0.0218 - acc: 0.9930 - val_loss: 0.0382 - val_acc: 0.9903\n",
      "Epoch 9/10\n",
      " - 52s - loss: 0.0191 - acc: 0.9936 - val_loss: 0.0299 - val_acc: 0.9925\n",
      "Epoch 10/10\n",
      " - 52s - loss: 0.0170 - acc: 0.9947 - val_loss: 0.0335 - val_acc: 0.9909\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.03347611008191407\n",
      "Test accuracy: 0.9909\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (1, 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5803017f1158>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtest_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpredicted_cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredicted_cat\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     \u001b[1;31m# means that we end up calculating it twice which we should avoid.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1864\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[1;32m--> 992\u001b[1;33m                                                      class_weight, batch_size)\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m         exception_prefix='input')\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    321\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (1, 784)"
     ]
    }
   ],
   "source": [
    "for x in range(1000):\n",
    "    test_image = test_images[x,:].reshape(1,784)\n",
    "    predicted_cat = model.predict(test_image).argmax()\n",
    "    label = test_labels[x].argmax()\n",
    "    if (predicted_cat != label):\n",
    "        plt.title('Prediction: %d Label: %d' % (predicted_cat, label))\n",
    "        plt.imshow(test_image.reshape([28,28]), cmap=plt.get_cmap('gray_r'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
